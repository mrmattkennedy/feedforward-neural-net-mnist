Purpose of hidden layers:
	Each layer can apply any function you want to the previous layer (usually a linear transformation followed by a squashing nonlinearity).
	The hidden layers' job is to transform the inputs into something that the output layer can use.
	The output layer transforms the hidden layer activations into whatever scale you wanted your output to be on.

	Multiple layers add multiple functions, more complex.
	Link: https://stats.stackexchange.com/questions/63152/what-does-the-hidden-layer-in-a-neural-network-compute
	
	One hidden layer is often enough. Rarely does performance increase for a second or third.
	Good rule of thumb is 1 hidden layer, neurons equal to average of input and output layer.
	Is each node a different tool? (i.e. one node for edges, one node for whatever, etc.)
		
	
	From https://towardsdatascience.com/exploring-activation-functions-for-neural-networks-73498da59b02:
		784 input neurons, one for each pixel.
Output:
	Machine mode or regressor mode
		Machine Mode returns a class label (one node, unless softmax, then one per class label)
		Regressor mode returns a value (one node)
	
Regularization:
	https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques/

Look into pruning and other techniques
Optimizations:
	Threading
	Matrix multiplications

NN has I/O layers, and n hidden layers
# nodes = len(i) + len(o) + sum(len(each hidden layer))
Weight between each node.

NN constructor - 
	# input nodes, **kwargs (H1, 5, H2, 3...), #output nodes
	OR
	User creates the nodes themselves and passes them in?
	
	How to get activation/squash functions for each node?
		# input nodes, **kwargs (H1.1, "Activation/Squash", H1.2, "Activation/?"
	
Node object with associated weight object
Node class vars:
	Name (I1, I2, B1, H12, H22, O1)
	Node #(1-# nodes)
	Layer (I, H1, H2, H..., O)
	Activation function (ability for different tools from each node).
	List of weights

Weight class vars:
	From-node
	To-node
	Weight value

1-4, 2-4, B1-4, 1-5, 2-5, B1-5, 4,6, B2-6
1	4	6
2	5
B1	B2




Main aspects
	Feed Forward
		Get the input data to start, and the weights.
		Create output list. This will have (# layers - 1 for input) nested arrays. Each nested array will be of shape (# inputs, # nodes in that layer).
			Ex: hidden layer is 3 nodes, 210 inputs = shape is 210,3. Output is 1 node, output list shape is 210,1.
		Loop through each weight in W. This will give an x,y array, where x is the # nodes in the previous layer, y is the # nodes in teh current layer.
			Ex: input is 2 nodes, hidden is 3, weights for that part will be shape 2x3. Hidden to output (1 node in output) is 3x1.
			
			While looping, do dot product of the entire input list (term for this? Feeding ALL inputs forward in this case), with that sublist w for those weights. This gives outputs for that layer.
			Run the new outputs through the activation function, and save those values to the output list. Set these new outputs to the new inputs as well for the next iteration.
			
	Back Propagation
		Need the prior outputs of each layer (the value after running through activation function), as well as the weights, and the results (training or test, whichever working on).
		Get the error for the output layer first - shape the output results the same as the outputs from feeding forward previously, and subtract. (results - output). This is the error for that layer
		Next, get the difference by multiplying the error by the derivative of the prior outputs. (delta = error * derivative(outputs)). This difference is the gradient for that layer.
		Last hidden layer (next layer down), that layer error = the dot product of the transposed weights for that layer by the delta for the output layer (prior layer).
		This is the cost function - diffefrence between network's output and the correct answers. This gives us how much each weight needs to adjust (roughly).
		Shape of delta: x,y where x is number of inputs and y is the number of nodes. 210 inputs x 1 nodes = 210,1
		
		From 3Blue1Brown video-cost of 1 training example - take output of network, what you wanted, add squares of differences between each componenet.
			Digit recognition example:
			Output 0-9 each have probability (with softmax), one has most probability. What you wanted was one digit to have 1.00 probability, with others having 0.00.
			Subtract the difference from each output node for what it was - what you wanted it to be, square root, and add.
				Do for every example, gives you total cost of network.
				One sample is the loss. Every example is cost.
			
			Backpropagation is showing how each input node would like to influence the previous layer (+/- some amount), and averaging them for each weight. 
			This is proportional to the negative gradient descent
			Cost function is the average of the loss function for every training sample in the whole set
		
		
		Gradient is saying "for each input, this is how I want the prior layer to change".		
		For updating weight we take that gradient value (1 per input in this case) and multiply by the input of each node on each input (3 nodes got 210 inputs).
		3 rows of 210: We're seeing how each inputs affected the output, and multiplying by their matching loss.
		
		xi = input i, ci = loss of input i
		alpha * sum(x1c1 + x2c2 +...+ xici).
		Sum how each input affected times how we need it to change, get the change we want.
		Add to weight.
		
		Good example:
		https://www.anotsorandomwalk.com/backpropagation-example-with-numbers-step-by-step/
		https://www.gormanalysis.com/blog/neural-networks-a-worked-example/
		Where we get our derivatives:
		https://stats.stackexchange.com/questions/235528/backpropagation-with-softmax-cross-entropy
	Update weights:
		Need the inputs for each layer starting point of the weights (input and hidden), as well as the deltas for the output layers (l1_delta, l2_delta), the weights, and the learning rate (alpha).
		Start at the end - weights between output layer and hidden layer = the weights + (learning rate * (the inputs transposed dot product multiplied by the output layer delta))





	Accuracy
	Loss
		Need to find a way to get user input on correct predictions.
		In this example, just the output > 0.5.
		What about OCR? 9 outputs, want 1 to be 1 and rest 0.
		output is [0.0, 0.0...1.0]
		target is same
		Sigmoid is adept at modeling probability, so use 0.5 as halfway between 0 and 1 for positive or negative output. Useful for classifying 2 classes, or 1 class among many.
		
		Sigmoid function - use acceptance threshold
		Softmax function - compare to what output should be (get the loss, see above 3Blue1Brown video)
		
		
Links:
https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw
capsule networks & squashing: https://medium.com/ai%C2%B3-theory-practice-business/understanding-hintons-capsule-networks-part-i-intuition-b4b559d1159b
							  https://www.sciencedirect.com/topics/computer-science/squashing-function
