for each weight in layer:
error * derivative(inputs would be output of that node) * output

60000 - # inputs
10 = num outputs
10 = num weights per output

sum each row? or col?


update in each weight = inputs transposed * derivative loss
=Xn.T * (Yhat-Y)n

change in all weights of node 22 is inputs transposed * deriv of loss
outputs of start node layer * loss gradient


gradient for weights in top layer is deriv CE * hidden value.
(y1 - t1) * hj, hj is the output of hidden layer node

500 x 10 weights - each output thinks each 500 weight
should change a certain way. Sum by column = loss for each class.
error gradient is inputs x outputs - 60000x10.
What are those 10? 
i is output unit, (yi-ti) = output - target summed

for 1 training example:
each output node says its weights should change this much.
I want the average of how each sample says the 500 weights should change
derivative loss for sample 1 is saying how slightly off sample 1 is.
add up ALL losses, get how wrong each one was. Average and apply.

do the average of each loss (10 values), and 

for each sample
output - (out - target) * hidden node value is gradient. (softmax derv)
middle - (out - target) * (weight) * (deriv of sigmoid)
out - target * weight * func prime for hidden node
error in hidden layer is 


sum of (derivative for output node i)(weight i,j)(hidden j * (1-hidden j)))


derivative of cost with respect to a weight (derivative by parts)=
dE/do1(cost), 
do1/dz01(output after and before function), 
dz01/dw7 (input * weight = z01)

WHY WE NEED BOTH DERIVATIVES:
we derive the error to get gradient.
Partial derivative requires the derivative of the activation func (softmax)
yi(oj-tj) for each weight, where y is the output of the start layer

get each col of last output layer (10 cols), * each col of gradient (10),
get avg (10 items)

error of hidden layer is sum (error * derivative * weight) for each
delta of weights is error _hl* derivative * input

